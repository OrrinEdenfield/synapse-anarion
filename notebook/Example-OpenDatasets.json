{
	"name": "Example-OpenDatasets",
	"properties": {
		"folder": {
			"name": "Examples"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "SynapseSpark",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "081e6796-229b-482a-9dcb-aeb662eb8981"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/e1bee07b-a025-41e2-ae0f-2291101bce96/resourceGroups/MiddleEarth/providers/Microsoft.Synapse/workspaces/anarion/bigDataPools/SynapseSpark",
				"name": "SynapseSpark",
				"type": "Spark",
				"endpoint": "https://anarion.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SynapseSpark",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.1",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Data visualization with Synapse Spark"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"In this tutorial, we'll use several different libraries to help us visualize the dataset. To do this analysis, import the following libraries:"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import matplotlib.pyplot as plt\r\n",
					"import seaborn as sns\r\n",
					"import pandas as pd"
				],
				"execution_count": 12
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Because the raw data is in a Parquet format, you can use the Spark context to pull the file into memory as a DataFrame directly. \r\n",
					"\r\n",
					"Create a Spark DataFrame by retrieving the data via the Open Datasets API. Here, we use the Spark DataFrame schema on read properties to infer the datatypes and schema."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from azureml.opendatasets import NycTlcYellow\r\n",
					"from datetime import datetime\r\n",
					"from dateutil import parser\r\n",
					"\r\n",
					"end_date = parser.parse('2018-06-06')\r\n",
					"start_date = parser.parse('2018-05-01')\r\n",
					"nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\r\n",
					"df = nyc_tlc.to_spark_dataframe()"
				],
				"execution_count": 13
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"After the data is read, we'll want to do some initial filtering to clean the dataset. We might remove unneeded columns and add columns that extract important information. \r\n",
					"\r\n",
					"In addition, we'll filter out anomalies within the dataset and create a temporary view that we can query with Spark SQL."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Filter the dataset \r\n",
					"from pyspark.sql.functions import *\r\n",
					"\r\n",
					"filtered_df = df.select('vendorID', 'passengerCount', 'tripDistance','paymentType', 'fareAmount', 'tipAmount'\\\r\n",
					"                                , date_format('tpepPickupDateTime', 'hh').alias('hour_of_day')\\\r\n",
					"                                , dayofweek('tpepPickupDateTime').alias('day_of_week')\\\r\n",
					"                                , dayofmonth(col('tpepPickupDateTime')).alias('day_of_month'))\\\r\n",
					"                            .filter((df.passengerCount > 0)\\\r\n",
					"                                & (df.tipAmount >= 0)\\\r\n",
					"                                & (df.fareAmount >= 1) & (df.fareAmount <= 250)\\\r\n",
					"                                & (df.tripDistance > 0) & (df.tripDistance <= 200))\r\n",
					"\r\n",
					"filtered_df.createOrReplaceTempView(\"taxi_dataset\")"
				],
				"execution_count": 14
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"By using this query, we want to understand how the average tip amounts have changed over the period we've selected. \r\n",
					"\r\n",
					"This query will also help us identify other useful insights, including the minimum/maximum tip amount per day and the average fare amount."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"SELECT \r\n",
					"    day_of_month\r\n",
					"    , MIN(tipAmount) AS minTipAmount\r\n",
					"    , MAX(tipAmount) AS maxTipAmount\r\n",
					"    , AVG(tipAmount) AS avgTipAmount\r\n",
					"    , AVG(fareAmount) as fareAmount\r\n",
					"FROM taxi_dataset \r\n",
					"GROUP BY day_of_month\r\n",
					"ORDER BY day_of_month ASC"
				],
				"execution_count": 15
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"To make development easier and less expensive, we'll downsample the dataset. We'll use the built-in Apache Spark sampling capability. \r\n",
					"\r\n",
					"In addition, both Seaborn and Matplotlib require a Pandas DataFrame or NumPy array. To get a Pandas DataFrame, use the toPandas() command to convert the DataFrame."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# To make development easier, faster, and less expensive, downsample for now\r\n",
					"sampled_taxi_df = filtered_df.sample(True, 0.001, seed=1234)\r\n",
					"\r\n",
					"# The charting package needs a Pandas DataFrame or NumPy array to do the conversion\r\n",
					"sampled_taxi_pd_df = sampled_taxi_df.toPandas()"
				],
				"execution_count": 16
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"We want to understand the distribution of tips in our dataset. We'll use Matplotlib to create a histogram that shows the distribution of tip amount and count. Based on the distribution, we can see that tips are skewed toward amounts less than or equal to $10."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Look at a histogram of tips by count by using Matplotlib\r\n",
					"\r\n",
					"ax1 = sampled_taxi_pd_df['tipAmount'].plot(kind='hist', bins=25, facecolor='lightblue')\r\n",
					"ax1.set_title('Tip amount distribution')\r\n",
					"ax1.set_xlabel('Tip Amount ($)')\r\n",
					"ax1.set_ylabel('Counts')\r\n",
					"plt.suptitle('')\r\n",
					"plt.show()"
				],
				"execution_count": 17
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Next, we want to understand the relationship between the tips for a given trip and the day of the week. \r\n",
					"\r\n",
					"Use Seaborn to create a box plot that summarizes the trends for each day of the week."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# View the distribution of tips by day of week using Seaborn\r\n",
					"ax = sns.boxplot(x=\"day_of_week\", y=\"tipAmount\",data=sampled_taxi_pd_df, showfliers = False)\r\n",
					"ax.set_title('Tip amount distribution per day')\r\n",
					"ax.set_xlabel('Day of Week')\r\n",
					"ax.set_ylabel('Tip Amount ($)')\r\n",
					"plt.show()"
				],
				"execution_count": 18
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Another hypothesis of ours might be that there's a positive relationship between the number of passengers and the total taxi tip amount. \r\n",
					"\r\n",
					"To verify this relationship, run the following code to generate a box plot that illustrates the distribution of tips for each passenger count."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# How many passengers tipped by various amounts \r\n",
					"ax2 = sampled_taxi_pd_df.boxplot(column=['tipAmount'], by=['passengerCount'])\r\n",
					"ax2.set_title('Tip amount by Passenger count')\r\n",
					"ax2.set_xlabel('Passenger count')\r\n",
					"ax2.set_ylabel('Tip Amount ($)')\r\n",
					"ax2.set_ylim(0,30)\r\n",
					"plt.suptitle('')\r\n",
					"plt.show()"
				],
				"execution_count": 19
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Last, we want to understand the relationship between the fare amount and the tip amount. Based on the results, we can see that there are several observations where people don't tip. \r\n",
					"\r\n",
					"However, we also see a positive relationship between the overall fare and tip amounts."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Look at the relationship between fare and tip amounts\r\n",
					"\r\n",
					"ax = sampled_taxi_pd_df.plot(kind='scatter', x= 'fareAmount', y = 'tipAmount', c='blue', alpha = 0.10, s=2.5*(sampled_taxi_pd_df['passengerCount']))\r\n",
					"ax.set_title('Tip amount by Fare amount')\r\n",
					"ax.set_xlabel('Fare Amount ($)')\r\n",
					"ax.set_ylabel('Tip Amount ($)')\r\n",
					"plt.axis([-2, 80, -2, 20])\r\n",
					"plt.suptitle('')\r\n",
					"plt.show()"
				],
				"execution_count": 20
			}
		]
	}
}